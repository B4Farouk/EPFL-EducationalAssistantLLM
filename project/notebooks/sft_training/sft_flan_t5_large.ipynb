{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ARCPH_bFVKJ","outputId":"834761ae-2d09-4423-f706-35623f2098bc","executionInfo":{"status":"ok","timestamp":1686471644525,"user_tz":-120,"elapsed":19491,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"beof3LP9FVDK","outputId":"a003cd01-9904-4a42-cee3-373def76b2c0","executionInfo":{"status":"ok","timestamp":1686472331516,"user_tz":-120,"elapsed":6,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/project-m3-notanagi/notebooks/sft_training\n"]}],"source":["%cd drive/MyDrive/project-m3-notanagi/notebooks/sft_training"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1QfJT_VBFSHJ","outputId":"16a1bf7e-aa0d-420c-f9a4-ebf98aa379bc","executionInfo":{"status":"ok","timestamp":1686471726033,"user_tz":-120,"elapsed":81139,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.39.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-sa7pvrvw\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-sa7pvrvw\n","  Resolved https://github.com/huggingface/transformers.git to commit 8f093fb799246f7dd9104ff44728da0c53a9f67a\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0.dev0)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0.dev0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.31.0.dev0)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0.dev0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0.dev0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0.dev0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0.dev0) (3.4)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.31.0.dev0-py3-none-any.whl size=7170473 sha256=1e00f1503e978e4d1f5d956eec957ba0930b640f531d774f0bdc0f2a75a88721\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-bjnbrh1_/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n","Successfully built transformers\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0.dev0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/peft.git\n","  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-eianopog\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-eianopog\n","  Resolved https://github.com/huggingface/peft.git to commit 189a6b8e357ecda05ccde13999e4c35759596a67\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (6.0)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (2.0.1+cu118)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (4.31.0.dev0)\n","Collecting accelerate (from peft==0.4.0.dev0)\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (0.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (16.0.5)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (0.15.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0.dev0) (2023.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.4.0.dev0) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.4.0.dev0) (1.3.0)\n","Building wheels for collected packages: peft\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=59308 sha256=a2dc3682317d5a3a35794f54eef3ed6c330b5398cad9a055f090d551cb5113ab\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-oy2yv_21/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n","Successfully built peft\n","Installing collected packages: accelerate, peft\n","Successfully installed accelerate-0.20.3 peft-0.4.0.dev0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/accelerate.git\n","  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-0wesvo25\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-0wesvo25\n","  Resolved https://github.com/huggingface/accelerate.git to commit 665d5180fcc01d5700f7a9aa3f9bdb75c6055dce\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0.dev0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0.dev0) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0.dev0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0.dev0) (6.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0.dev0) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.21.0.dev0) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.21.0.dev0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.21.0.dev0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.21.0.dev0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.21.0.dev0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.21.0.dev0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.21.0.dev0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.21.0.dev0) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate==0.21.0.dev0) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate==0.21.0.dev0) (1.3.0)\n","Building wheels for collected packages: accelerate\n","  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for accelerate: filename=accelerate-0.21.0.dev0-py3-none-any.whl size=228089 sha256=24522eec4e2a446283e7fede71cc06e3ba8029f7176d6ebf9cf07618f6238fd3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-xmj9nukm/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\n","Successfully built accelerate\n","Installing collected packages: accelerate\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.20.3\n","    Uninstalling accelerate-0.20.3:\n","      Successfully uninstalled accelerate-0.20.3\n","Successfully installed accelerate-0.21.0.dev0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Collecting responses<0.19 (from datasets)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"]}],"source":["!pip install sentencepiece\n","!pip install -U bitsandbytes\n","!pip install -U git+https://github.com/huggingface/transformers.git \n","!pip install -U git+https://github.com/huggingface/peft.git\n","!pip install -U git+https://github.com/huggingface/accelerate.git\n","!pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-zSUEm4IFSHM","outputId":"5c2801ad-dbb0-44c1-b5fc-93e1e6dd7e0b","executionInfo":{"status":"ok","timestamp":1686472351593,"user_tz":-120,"elapsed":16091,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===================================BUG REPORT===================================\n","Welcome to bitsandbytes. For bug reports, please run\n","\n","python -m bitsandbytes\n","\n"," and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n","================================================================================\n","bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n","CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n","CUDA SETUP: Detected CUDA version 118\n","CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-y3ibodhbuain --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n","Either way, this might cause trouble in the future:\n","If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n","  warn(msg)\n"]}],"source":["import sys\n","MODULE_PATHS = [\"../../modules\"]\n","for module_path in MODULE_PATHS:\n","    if module_path not in sys.path:\n","        sys.path.append(module_path)\n","\n","import json\n","\n","\n","import torch\n","from fine_tuning import build_finetuning_hf_dataset\n","\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n","from datasets import load_dataset\n","from transformers import BitsAndBytesConfig\n","\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","source":["# Loading model for training using LoRa"],"metadata":{"id":"rf6U3wHQCwhM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLAt_CxdFSHM"},"outputs":[],"source":["model_id = \"google/flan-t5-large\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NRLTF38FSHN","outputId":"f5341d72-52a2-4a06-9adc-a16e9772e292","executionInfo":{"status":"ok","timestamp":1686472371599,"user_tz":-120,"elapsed":6928,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 4,718,592 || all params: 787,868,672 || trainable%: 0.5989059049678777\n"]}],"source":["from peft import TaskType\n","\n","#model.gradient_checkpointing_enable()\n","#model = prepare_model_for_kbit_training(model)\n","\n","config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=[\"q\", \"v\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_2_SEQ_LM\n",")\n","model = get_peft_model(model, config)\n","\n","# print number of trainable parameters\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09yr0Rc2GT3U","outputId":"b940466f-0be2-4e2d-f6a7-2b9e210efb60","executionInfo":{"status":"ok","timestamp":1686472371600,"user_tz":-120,"elapsed":25,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.float32 787868672 1.0\n"]}],"source":["# Verifying the datatypes.\n","dtypes = {}\n","for _, p in model.named_parameters():\n","    dtype = p.dtype\n","    if dtype not in dtypes:\n","        dtypes[dtype] = 0\n","    dtypes[dtype] += p.numel()\n","total = 0\n","for k, v in dtypes.items():\n","    total += v\n","for k, v in dtypes.items():\n","    print(k, v, v / total)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"e3NmQ15AGT3V","colab":{"base_uri":"https://localhost:8080/","height":73,"referenced_widgets":["d2675af2d1394db9a3597133d4b78257","146d5c8f53a44f17a0ce2ddd07eb0d98","5df00c19c0c74a4cbd68fe36e1601b57","021dbe8e083b4c77b839ae7d49b16a23","f698a0d5ad1842b7b31308ee258dbf02","e36e28b26352494ebf820f137dbcc010","9879916d3b324d90809d9d48b71a12f4","41a246b007d44ad49959943656fab5ba","0b14ea7a03ed4b24952578443bbe2c34","21931219892441a78858cebc2e8fdd6f","b4bc7b20ff9947cc9844b3377815166b","5f8de51cb6e94cf08e846461bdcdde9d","a28a577d37b34c6184310f7421091316","a9a49f1c85fe48ddaee41bbc177bf6c6","93da3f882a3845a292135d251d60fe92","93a58b9a51be4778b68dc5a879ed6484","a51d44c49e614a6084eade6643070301","da5502140b88479ea841c91177f2f98e","25bd4fe5350a447fb628ac779ed591db","07eac62c9635414f9bfbb426304e856c","7396daf1a8724ea68eaf595af3e18395","73cb5210cefb40f28277d8e5c36ca37e","81a1fb2bb45a4385a66524bda5725a2b","7f5001537aa24adc83ca8bc584408540","cfb7633cdc3c40588b7b1cd86d9f8d79","b66635cab734414782927f4cf73a884e","9eb49d006e3044f98f8f40d443b3a8be","f3b8d347d9754e40a070166e8d402bd1","9610f2480fe14f458e09f87b648ecc02","c06d57da18434710b9dcd97a0c027142","34f61acb19ff467a95b4a7d228e5e365","48de188c7fa044b8b0d8b3ec0214757a","39a68bce2a5e4289968c446dc17695cf"]},"outputId":"27703b85-7d3c-4ae2-b320-a9de39266ac8","executionInfo":{"status":"ok","timestamp":1686472396805,"user_tz":-120,"elapsed":25226,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/10647 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2675af2d1394db9a3597133d4b78257"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (737 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["dropped examples:  1721\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/8926 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8de51cb6e94cf08e846461bdcdde9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/8926 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a1fb2bb45a4385a66524bda5725a2b"}},"metadata":{}}],"source":["max_input_length = 512\n","# change max input length of tokenizer to max_input_length\n","tokenizer.model_max_length = max_input_length\n","with open(\"../../submission_items/gen_dataset_NotAnAGI.json\", \"rb\") as reader:\n","    dataset = build_finetuning_hf_dataset(json.load(reader), tokenizer=tokenizer, max_input_length=max_input_length)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_-SrItLKVDP","outputId":"32621dc3-dce8-4809-adf6-2622c3c02906","executionInfo":{"status":"ok","timestamp":1686472396806,"user_tz":-120,"elapsed":15,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 8479\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 447\n","    })\n","})"]},"metadata":{},"execution_count":7}],"source":["dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMswsTqfGT3X","outputId":"68cb5c26-2f1a-46df-fed1-223b38e7fd22","executionInfo":{"status":"ok","timestamp":1686472397193,"user_tz":-120,"elapsed":396,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([  101,    33,   787,    24,     8,  1612, 10356,   358,  2284,  5354,\n","         5096,     7,    13,  2850,     6,    84,   598,    34,  1416,    44,\n","          334,   386, 12096,  2850,    16,     8,   787,  1448,     5,    86,\n","           48,   495,     6,    62,    33,   787,     8,  1448,    96,  3007,\n","         7310,  1280,   264,     6,    62,   174,    12,  1099,     8,   826,\n","         5354,  5096,     7,    10,     3,    18,  8524,   115,     3,    18,\n","            3, 21207,     3,    18,     3,   115,   115,    40,     3,    18,\n","            3,  2296,   242,   284,    13,   175,  5354,  5096,     7,     6,\n","           62,   174,    12, 29216,     8, 15834,    13,    34, 12770,    12,\n","         1612,   301,     3,   390,    30,     8,   787,  9551,  2197,     5,\n","          101,    54,   169,     8,  3741,  3356,    13, 15834,    12,   103,\n","           48,    10,  1514,  3229,   276,   599,     2,  6327,     2,   519,\n","           18,  5096,  9175,   434,     2,    61,  3274,   276,   599,     2,\n","         6327,     2, 14965,  1848,  9175, 14672,   192,  2850,     6,   434,\n","            2,    61,     3,     2,   715,     7,   276,   599,     2,  6327,\n","            2, 12091,  1848,  9175, 14672,    11,  1025,  1848,     6,   434,\n","            2,    61,     3,     2,   715,     7,   276,   599,     2,  6327,\n","            2, 14672,  1848,  9175, 12091,    11,  1025,  1848,     6,   434,\n","            2,    61,  1514,  3229,   242,   677,     6,   752,    31,     7,\n","        29216,     8, 15834,    13,    96,  3007,   115,   121, 12770,    12,\n","         1612,   301,    10,  1514,  3229,   276,   599,     2,  6327,     2,\n","         3007,   115,  9175,   434,     2,    61,  3274,   276,   599,     2,\n","         6327,     2,   115,  9175,   434,     2,    61,     3,     2,   715,\n","            7,   276,   599,     2,  6327,     2,    76,  9175,  3007,     6,\n","          434,     2,    61,     3,     2,   715,     7,   276,   599,     2,\n","         6327,     2,   115,  9175,    76,   115,     6,   434,     2,    61,\n","         1514,  3229,     3,  3626,     8,   787,  9551,  2197,     6,    62,\n","          129,    10,  1514,  3229,   276,   599,     2,  6327,     2,  3007,\n","          115,  9175,   434,     2,    61,  3274,   431,     3,     2,   715,\n","            7,   335,     2,  4949,     2,     3,     2,   715,     7,   314,\n","            3,     2,   715,     7,   335,     2,  4949,     2,     3,     2,\n","          715,     7,   489,     3,     2,   715,     7,   335,     2,  4949,\n","            2,  3274,  1300,  3651,     3,     2,   715,     7,   335,     2,\n","         4525,     2,  1514,  3229,     3, 15209,     6,    62,    54, 29216,\n","            8,  9551,  2197,    21,     8,   119,  5354,  5096,     7,    10,\n","            3,    18,     3,     2,   599,   276,   599,     2,  6327,     2,\n","        21207,  9175,   434,     2,    61,  3274,   431,     3,     2,   715,\n","            7,   335,     2,  4949,     2,     3,     2,   715,     7,   209,\n","            3,     2,   715,     7,   335,     2,  4278,     2,     3,     2,\n","          715,     7,   314,     3,     2,   715,     7,   335,     2,  4949,\n","            2,  3274,     3, 17638,     3,     2,   715,     7,   335,     2,\n","         7141,     2,     3,     2,    61,     3,    18,     3,     2,   599,\n","          276,   599,     2,  6327,     2,   115,   115,    40,  9175,   434,\n","            2,    61,  3274,   489,     3,     2,   715,     7,   335,     2,\n","         4949,     2,     3,     2,   715,     7,   220,     3,     2,   715,\n","            7,   335,     2,  4278,     2,     3,     2,   715,     7,   204,\n","            3,     2,   715,     7,   335,     2,  4949,     2,  3274,     3,\n","        19765,     3,     2,   715,     7,   335,     2,  6039,     2,     3,\n","            2,    61,     3,    18,     3,     2,   599,   276,   599,     2,\n","         6327,     2,  2296,  9175,   434,     2,    61,  3274,   305,     3,\n","            2,   715,     7,   335,     2,  3486,     2,     3,     2,   715,\n","            7,     1])"]},"metadata":{},"execution_count":8}],"source":["dataset[\"train\"][1][\"labels\"]"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzHvGaTuUF04","outputId":"5f11700e-1b70-4f8b-cbc5-2a3787a10641","executionInfo":{"status":"ok","timestamp":1686472397194,"user_tz":-120,"elapsed":11,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["512"]},"metadata":{},"execution_count":9}],"source":["len(dataset[\"train\"][1][\"labels\"])"]},{"cell_type":"markdown","metadata":{"id":"0JmWC8XbGT3X"},"source":["# Before training"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-W7er51GT3Y","outputId":"e77f3f2f-79a9-430f-ca92-29a4850626d5","executionInfo":{"status":"ok","timestamp":1686471833912,"user_tz":-120,"elapsed":19448,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["RESULT:  0\n","The redshift of galaxies with a very high redshift is about 5. So the galaxy has to be at least 5 times brighter than the Earth. If the R filter is used, then there are no signs of stellar formation. Thus, it can be concluded that (B).\n","INPUT:  0\n","Question: We look for signs of the stellar formation in very old galaxies with the redshift of about 5. So one wants to observe the H alpha emission line in the gas clouds made of ionised hydrogen. The R filter (covering the wavelength range from 6000 to 7000 Angström) seems to be the right filter to use. Unfortunately, there is nothing in the images. Why? (A) Because of the cosmological shift towards smaller wavelengths at high redshift, one has to observe the H alpha line in an ultraviolet filter. (B) Because of the shift towards longer wavelengths at high redshift, one has to observe the H alpha line in an infrared filter. (C) Probably the R filter is dirty, one has to clean it. (D) The galaxies at very high redshift are not bright enough to be observed. Answer: Let’s think step by step: Answer: \n","LABEL:  0\n","At high redshift, there is a cosmological shift towards longer wavelengths, meaning that the H alpha emission line will be shifted towards longer wavelengths. Therefore, (A) is not correct. It is also not likely that the filter is dirty, so (C) can be ruled out. The correct answer is (B), because the shift towards longer wavelengths means that the H alpha line will be shifted towards the infrared part of the spectrum, and therefore one needs an infrared filter to observe it. Finally, (D) is not necessarily correct, as there are ways to observe faint galaxies even at high redshifts, such as using longer exposure times or larger telescopes.\n","--------------------------------------------------------\n","RESULT:  1\n","begaligned T_Aleft( begin'pmatrixes a &b, b & c end-pmatrixright) &= A beg[0] beg[1] % 2 & 0  0 â€“ 3 & 0 à 1 & -2 & 1 end>matrix beg.\n","INPUT:  1\n","Question: Peut-tu mieux expliquer ton raisonnement? Answer: Bien sûr! Tout d'abord, rappelons que l'application linéaire $T_A:Vto V$ est définie par $T_A(M)=AM-MA$, où $A$ est la matrice donnée dans l'énoncé et $V$ est le sous-espace vectoriel de $M_2times 2(mathbbR)$ constitué des matrices symétriques. Ensuite, on nous demande de calculer l'image d'une matrice symétrique générique de $V$ sous l'action de $T_A$. Cette matrice est de la forme $beginpmatrix a & b  b & c endpmatrix$, où $a,b,c$ sont des réels quelconques. Ainsi, on calcule: $$ beginaligned T_Aleft( beginpmatrix a & b  b & c endpmatrixright) &= A beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix A  &= beginpmatrix -2 & 4 & 0  0 & 3 & 0  1 & -2 & 1 endpmatrix beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix beg\n","LABEL:  1\n","Bien sûr, voici la réponse sous forme de LaTeX: La réponse correcte est: $$T_Aleft( beginpmatrix a & b  b & c endpmatrixright) = beginpmatrix -2a+4b & a-2b+c  a-2b+c & 3c endpmatrix.$$\n","--------------------------------------------------------\n","RESULT:  2\n","(f',f'':mathbbRto>mathbbr)\n","INPUT:  2\n","Question: Soit (f:mathbbRtomathbbR) une fonction deux fois dérivable sur (mathbbR) et (f',f'':mathbbRtomathbbR) ses fonctions dérivées première et deuxième. Alors 1) (f) est continue sur (mathbbR) 2) (f') est continue sur (mathbbR) 3) (f'') est continue sur (mathbbR) 4) pour tout (x_0inmathbbR) (displaystylelim_xto x_0f'(x)) existe 5) pour tout (x_0inmathbbR) (displaystylelim_xto x_0fracf'(x)-f'(x_0)x-x_0) existe Answer: \n","LABEL:  2\n","Tout d'abord, nous savons que la dérivabilité de (f) implique sa continuité. Donc, la proposition 1 est vraie car (f) est deux fois dérivable, donc elle est aussi dérivable et donc continue. Ensuite, la proposition 2 est vraie car si (f) est deux fois dérivable, alors sa dérivée première (f') est dérivable. Et nous savons que la dérivabilité implique la continuité. La proposition 3 n'est pas nécessairement vraie. Le fait que (f'') existe ne garantit pas sa continuité. Par exemple, la fonction (f(x)=|x|) est deux fois dérivable sur (mathbbR), mais sa dérivée seconde n'est pas continue en (x=0). La proposition 4 est vraie car si (f') est dérivable en (x_0), alors elle est continue en (x_0). Enfin, la proposition 5 est vraie car si (f'') existe en (x_0), alors la limite (displaystylelim_xto x_0fracf'(x)-f'(x_0)x-x_0) existe et est égale à (f''(x_0)). Cela correspond à la définition de la dérivée seconde de (f) en (x_0).\n","--------------------------------------------------------\n","RESULT:  3\n","langle nrangle = left.fracpartial Gparcial zright|_'z=1=[/mathjaxinline]\n","INPUT:  3\n","Question: In the case where [mathjaxinline]alpha=1, beta=0.1, [/mathjaxinline][mathjaxinline]langle nrangle=left.fracpartial Gpartial zright|_z=1=[/mathjaxinline] Answer: \n","LABEL:  3\n","Assuming that [mathjaxinline]G(z)[/mathjaxinline] is the generating function for the sequence [mathjaxinline]n_k[/mathjaxinline], where [mathjaxinline]n_k[/mathjaxinline] denotes the number of occurrences of some event in the k-th trial, we have: [mathjaxinline]G(z) = sum_k=0infty n_k zk[/mathjaxinline] Given that [mathjaxinline]langle nrangle=left.fracpartial Gpartial zright|_z=1[/mathjaxinline], we can differentiate [mathjaxinline]G(z)[/mathjaxinline] with respect to [mathjaxinline]z[/mathjaxinline] and then set [mathjaxinline]z=1[/mathjaxinline] to obtain: [mathjaxinline]langle nrangle = left.fracpartial Gpartial zright|_z=1 = sum_k=1infty k n_k[/mathjaxinline] Now, using the fact that [mathjaxinline]alpha=1[/mathjaxinline] and [mathjaxinline]beta=0.1[/mathjaxinline], we can write: [mathjaxinline]n_k = frac(k-1)!(1-alpha-beta)k!alphakbeta1-1 = frac110kalphak[/mathjaxinline] Substituting this expression for [mathjaxinline]n_k[/mathjaxinline] into the equation for [mathjaxinline]langle nrangle[/mathjaxinline], we get: [\n","--------------------------------------------------------\n","RESULT:  4\n","hyperparameters\n","INPUT:  4\n","Question: Classify the following quantity as parameters (P) or hyperparameters (H) of a learning algorithm: The cluster assignments in a k-means algorithm. Answer: \n","LABEL:  4\n","In a k-means algorithm, the cluster assignments are parameters (P) rather than hyperparameters (H). Here's why: - Parameters are variables that are learned during the training process. In the case of k-means, the cluster assignments (i.e., which data points belong to which cluster) are determined during the training process and are updated iteratively until convergence. Therefore, they are considered parameters of the algorithm. - Hyperparameters, on the other hand, are values that are set before the training process begins and are not learned during training. Examples of hyperparameters in k-means include the number of clusters (k), the initialization method, and the convergence criteria. These hyperparameters are set before the training process begins and are not updated during training. So, in summary, the cluster assignments in a k-means algorithm are parameters (P) of the algorithm rather than hyperparameters (H).\n","--------------------------------------------------------\n"]}],"source":["def generate_example(gen_model, tokenizer, dataset, num_examples=5):\n","    for i in range(num_examples):\n","        example_input = dataset[\"test\"][i][\"input_ids\"].to(\"cuda\")\n","        example_label = dataset[\"test\"][i][\"labels\"].to(\"cuda\")\n","\n","        output = gen_model.generate(\n","            input_ids=example_input.unsqueeze(0),\n","            max_new_tokens=1000,\n","            temperature=0.9,\n","            top_p=0.9,\n","            num_return_sequences=1,\n","            repetition_penalty=100.0\n","\n","        )\n","\n","        print(\"RESULT: \", i)\n","        print(tokenizer.decode(output[0], skip_special_tokens=True))\n","        # decode input and label\n","        print(\"INPUT: \", i)\n","        print(tokenizer.decode(example_input, skip_special_tokens=True))\n","        print(\"LABEL: \", i)\n","        print(tokenizer.decode(example_label, skip_special_tokens=True))\n","        print(\"--------------------------------------------------------\")\n","\n","generate_example(model, tokenizer, dataset, num_examples=5)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"l6dT-Jz9FSHO","executionInfo":{"status":"ok","timestamp":1686472397195,"user_tz":-120,"elapsed":9,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","training_args = TrainingArguments(\n","    #auto_find_batch_size=True,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    num_train_epochs=1,\n","    learning_rate=1e-3,\n","    #learning_rate=5e-5,\n","    fp16=False,\n","    save_total_limit=4,\n","    logging_steps=5,\n","    eval_steps=100,\n","    output_dir=\"./outputs\",\n","    save_strategy='epoch',\n","    #optim=\"paged_adamw_8bit\",\n","    lr_scheduler_type = 'cosine',\n","    warmup_ratio = 0.1,\n",")\n","class ConversationTrainer(Trainer):    \n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        assert return_outputs == False, \"we did not implement return_outputs=True in RewardTrainer.compute_loss\"\n","        \n","        #print(\"inputs: \", inputs)\n","        input_ids = inputs[\"input_ids\"]\n","        labels = inputs[\"labels\"]\n","        #print(\"labels: \", labels)\n","        print(\"len inputs: \", input_ids.shape)\n","        print(\"len labels: \", labels.shape)\n","        loss, outputs = Trainer.compute_loss(self, model, inputs, return_outputs=True)\n","        print(\"loss: \", loss)\n","        print(\"outputs: \", outputs)\n","        wefwefw\n","        return loss\n","\n","\n","\n","        \n","\n","\n","# we want to ignore tokenizer pad token in the loss\n","label_pad_token_id = -100\n","# Data collator\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer,\n","    model=model,\n","    padding=\"max_length\",\n","    label_pad_token_id=label_pad_token_id,\n","    pad_to_multiple_of=8\n",")\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    args=training_args,\n","    data_collator=data_collator\n",")\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-Ub9IfSCGT3Z","outputId":"5c4cdb89-e998-47d3-e5c0-caefdab1d008","executionInfo":{"status":"ok","timestamp":1686477368705,"user_tz":-120,"elapsed":4971518,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2119' max='2119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2119/2119 1:22:48, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>1.981500</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.981500</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>2.104500</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.087000</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>1.935300</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.745200</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>2.106100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.778500</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>1.805100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.945200</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>1.854800</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.835300</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>1.803000</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>1.846500</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.637800</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.823500</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>1.845300</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>1.791100</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>1.979100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.729100</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>1.722900</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>1.544600</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>1.963900</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>1.783900</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>1.781100</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>1.651500</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>1.671700</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>1.942900</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>1.727300</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.762500</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>1.686200</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>1.532700</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>1.695300</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>1.559900</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>1.806800</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>1.838100</td>\n","    </tr>\n","    <tr>\n","      <td>185</td>\n","      <td>1.686300</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>1.854800</td>\n","    </tr>\n","    <tr>\n","      <td>195</td>\n","      <td>1.611900</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.734900</td>\n","    </tr>\n","    <tr>\n","      <td>205</td>\n","      <td>1.801500</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>1.641500</td>\n","    </tr>\n","    <tr>\n","      <td>215</td>\n","      <td>1.896700</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>1.637000</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.734800</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>1.772800</td>\n","    </tr>\n","    <tr>\n","      <td>235</td>\n","      <td>1.947600</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>1.766600</td>\n","    </tr>\n","    <tr>\n","      <td>245</td>\n","      <td>1.651200</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.813900</td>\n","    </tr>\n","    <tr>\n","      <td>255</td>\n","      <td>1.494100</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>1.646400</td>\n","    </tr>\n","    <tr>\n","      <td>265</td>\n","      <td>1.641700</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>1.746700</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>1.756600</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>1.782500</td>\n","    </tr>\n","    <tr>\n","      <td>285</td>\n","      <td>1.860900</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>1.852900</td>\n","    </tr>\n","    <tr>\n","      <td>295</td>\n","      <td>1.883800</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.615200</td>\n","    </tr>\n","    <tr>\n","      <td>305</td>\n","      <td>1.677800</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>1.555900</td>\n","    </tr>\n","    <tr>\n","      <td>315</td>\n","      <td>1.510500</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>1.762500</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>1.834300</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>1.642400</td>\n","    </tr>\n","    <tr>\n","      <td>335</td>\n","      <td>1.759300</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>1.701400</td>\n","    </tr>\n","    <tr>\n","      <td>345</td>\n","      <td>1.687100</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.710500</td>\n","    </tr>\n","    <tr>\n","      <td>355</td>\n","      <td>1.595300</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>1.698500</td>\n","    </tr>\n","    <tr>\n","      <td>365</td>\n","      <td>1.652000</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>1.678500</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>1.604100</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>1.780800</td>\n","    </tr>\n","    <tr>\n","      <td>385</td>\n","      <td>1.701700</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>1.605900</td>\n","    </tr>\n","    <tr>\n","      <td>395</td>\n","      <td>1.617900</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.864900</td>\n","    </tr>\n","    <tr>\n","      <td>405</td>\n","      <td>1.755600</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>1.738000</td>\n","    </tr>\n","    <tr>\n","      <td>415</td>\n","      <td>1.488000</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>1.774400</td>\n","    </tr>\n","    <tr>\n","      <td>425</td>\n","      <td>1.674400</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>1.708200</td>\n","    </tr>\n","    <tr>\n","      <td>435</td>\n","      <td>1.470100</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>1.652400</td>\n","    </tr>\n","    <tr>\n","      <td>445</td>\n","      <td>1.830600</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.505100</td>\n","    </tr>\n","    <tr>\n","      <td>455</td>\n","      <td>1.786600</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>1.681400</td>\n","    </tr>\n","    <tr>\n","      <td>465</td>\n","      <td>1.574700</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>1.570400</td>\n","    </tr>\n","    <tr>\n","      <td>475</td>\n","      <td>1.633400</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>1.488400</td>\n","    </tr>\n","    <tr>\n","      <td>485</td>\n","      <td>1.807100</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>1.707900</td>\n","    </tr>\n","    <tr>\n","      <td>495</td>\n","      <td>1.579900</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.794800</td>\n","    </tr>\n","    <tr>\n","      <td>505</td>\n","      <td>1.808100</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>1.748600</td>\n","    </tr>\n","    <tr>\n","      <td>515</td>\n","      <td>1.662300</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>1.375100</td>\n","    </tr>\n","    <tr>\n","      <td>525</td>\n","      <td>1.686800</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>1.306500</td>\n","    </tr>\n","    <tr>\n","      <td>535</td>\n","      <td>1.458900</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>1.827500</td>\n","    </tr>\n","    <tr>\n","      <td>545</td>\n","      <td>1.720300</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>1.703000</td>\n","    </tr>\n","    <tr>\n","      <td>555</td>\n","      <td>1.531500</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>1.628200</td>\n","    </tr>\n","    <tr>\n","      <td>565</td>\n","      <td>1.580900</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>1.605300</td>\n","    </tr>\n","    <tr>\n","      <td>575</td>\n","      <td>1.638000</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>1.617900</td>\n","    </tr>\n","    <tr>\n","      <td>585</td>\n","      <td>1.492200</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>1.596200</td>\n","    </tr>\n","    <tr>\n","      <td>595</td>\n","      <td>1.493900</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.780600</td>\n","    </tr>\n","    <tr>\n","      <td>605</td>\n","      <td>1.704900</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>1.761700</td>\n","    </tr>\n","    <tr>\n","      <td>615</td>\n","      <td>1.758600</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>1.684900</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>1.526000</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>1.603300</td>\n","    </tr>\n","    <tr>\n","      <td>635</td>\n","      <td>1.732000</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>1.490300</td>\n","    </tr>\n","    <tr>\n","      <td>645</td>\n","      <td>1.830000</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>1.532000</td>\n","    </tr>\n","    <tr>\n","      <td>655</td>\n","      <td>1.368800</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>1.322700</td>\n","    </tr>\n","    <tr>\n","      <td>665</td>\n","      <td>1.792200</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>1.673800</td>\n","    </tr>\n","    <tr>\n","      <td>675</td>\n","      <td>1.509000</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>1.622800</td>\n","    </tr>\n","    <tr>\n","      <td>685</td>\n","      <td>1.978300</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>1.559600</td>\n","    </tr>\n","    <tr>\n","      <td>695</td>\n","      <td>1.742400</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.768400</td>\n","    </tr>\n","    <tr>\n","      <td>705</td>\n","      <td>1.685100</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>1.560200</td>\n","    </tr>\n","    <tr>\n","      <td>715</td>\n","      <td>1.504800</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>1.528200</td>\n","    </tr>\n","    <tr>\n","      <td>725</td>\n","      <td>1.702600</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>1.497900</td>\n","    </tr>\n","    <tr>\n","      <td>735</td>\n","      <td>1.561600</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>1.595500</td>\n","    </tr>\n","    <tr>\n","      <td>745</td>\n","      <td>1.583800</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.666400</td>\n","    </tr>\n","    <tr>\n","      <td>755</td>\n","      <td>1.605000</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>1.569200</td>\n","    </tr>\n","    <tr>\n","      <td>765</td>\n","      <td>1.595900</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>1.393400</td>\n","    </tr>\n","    <tr>\n","      <td>775</td>\n","      <td>1.570300</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>1.512300</td>\n","    </tr>\n","    <tr>\n","      <td>785</td>\n","      <td>1.656000</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>1.676500</td>\n","    </tr>\n","    <tr>\n","      <td>795</td>\n","      <td>1.483800</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.441100</td>\n","    </tr>\n","    <tr>\n","      <td>805</td>\n","      <td>1.409000</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>1.644800</td>\n","    </tr>\n","    <tr>\n","      <td>815</td>\n","      <td>1.827300</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>1.526100</td>\n","    </tr>\n","    <tr>\n","      <td>825</td>\n","      <td>1.483700</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>1.422900</td>\n","    </tr>\n","    <tr>\n","      <td>835</td>\n","      <td>1.529900</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>1.572000</td>\n","    </tr>\n","    <tr>\n","      <td>845</td>\n","      <td>1.562700</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>1.544300</td>\n","    </tr>\n","    <tr>\n","      <td>855</td>\n","      <td>1.540200</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>1.625600</td>\n","    </tr>\n","    <tr>\n","      <td>865</td>\n","      <td>1.555600</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>1.705000</td>\n","    </tr>\n","    <tr>\n","      <td>875</td>\n","      <td>1.483300</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>1.647600</td>\n","    </tr>\n","    <tr>\n","      <td>885</td>\n","      <td>1.354100</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>1.679800</td>\n","    </tr>\n","    <tr>\n","      <td>895</td>\n","      <td>1.405700</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.520400</td>\n","    </tr>\n","    <tr>\n","      <td>905</td>\n","      <td>1.603300</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>1.716700</td>\n","    </tr>\n","    <tr>\n","      <td>915</td>\n","      <td>1.625100</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>1.583700</td>\n","    </tr>\n","    <tr>\n","      <td>925</td>\n","      <td>1.631700</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>1.701700</td>\n","    </tr>\n","    <tr>\n","      <td>935</td>\n","      <td>1.659800</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>1.629100</td>\n","    </tr>\n","    <tr>\n","      <td>945</td>\n","      <td>1.500300</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>1.758500</td>\n","    </tr>\n","    <tr>\n","      <td>955</td>\n","      <td>1.622700</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>1.653900</td>\n","    </tr>\n","    <tr>\n","      <td>965</td>\n","      <td>1.554900</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>1.700800</td>\n","    </tr>\n","    <tr>\n","      <td>975</td>\n","      <td>1.636200</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>1.705100</td>\n","    </tr>\n","    <tr>\n","      <td>985</td>\n","      <td>1.672200</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>1.556500</td>\n","    </tr>\n","    <tr>\n","      <td>995</td>\n","      <td>1.697000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.625000</td>\n","    </tr>\n","    <tr>\n","      <td>1005</td>\n","      <td>1.476400</td>\n","    </tr>\n","    <tr>\n","      <td>1010</td>\n","      <td>1.553500</td>\n","    </tr>\n","    <tr>\n","      <td>1015</td>\n","      <td>1.675700</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>1.627000</td>\n","    </tr>\n","    <tr>\n","      <td>1025</td>\n","      <td>1.692200</td>\n","    </tr>\n","    <tr>\n","      <td>1030</td>\n","      <td>1.621300</td>\n","    </tr>\n","    <tr>\n","      <td>1035</td>\n","      <td>1.673300</td>\n","    </tr>\n","    <tr>\n","      <td>1040</td>\n","      <td>1.808600</td>\n","    </tr>\n","    <tr>\n","      <td>1045</td>\n","      <td>1.679000</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>1.574700</td>\n","    </tr>\n","    <tr>\n","      <td>1055</td>\n","      <td>1.636600</td>\n","    </tr>\n","    <tr>\n","      <td>1060</td>\n","      <td>1.512100</td>\n","    </tr>\n","    <tr>\n","      <td>1065</td>\n","      <td>1.475500</td>\n","    </tr>\n","    <tr>\n","      <td>1070</td>\n","      <td>1.594400</td>\n","    </tr>\n","    <tr>\n","      <td>1075</td>\n","      <td>1.704700</td>\n","    </tr>\n","    <tr>\n","      <td>1080</td>\n","      <td>1.700700</td>\n","    </tr>\n","    <tr>\n","      <td>1085</td>\n","      <td>1.576200</td>\n","    </tr>\n","    <tr>\n","      <td>1090</td>\n","      <td>1.769700</td>\n","    </tr>\n","    <tr>\n","      <td>1095</td>\n","      <td>1.552300</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>1.652800</td>\n","    </tr>\n","    <tr>\n","      <td>1105</td>\n","      <td>1.447200</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>1.797200</td>\n","    </tr>\n","    <tr>\n","      <td>1115</td>\n","      <td>1.383900</td>\n","    </tr>\n","    <tr>\n","      <td>1120</td>\n","      <td>1.555200</td>\n","    </tr>\n","    <tr>\n","      <td>1125</td>\n","      <td>1.499300</td>\n","    </tr>\n","    <tr>\n","      <td>1130</td>\n","      <td>1.518200</td>\n","    </tr>\n","    <tr>\n","      <td>1135</td>\n","      <td>1.676500</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>1.680500</td>\n","    </tr>\n","    <tr>\n","      <td>1145</td>\n","      <td>1.500300</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>1.778900</td>\n","    </tr>\n","    <tr>\n","      <td>1155</td>\n","      <td>1.673500</td>\n","    </tr>\n","    <tr>\n","      <td>1160</td>\n","      <td>1.356100</td>\n","    </tr>\n","    <tr>\n","      <td>1165</td>\n","      <td>1.321100</td>\n","    </tr>\n","    <tr>\n","      <td>1170</td>\n","      <td>1.535700</td>\n","    </tr>\n","    <tr>\n","      <td>1175</td>\n","      <td>1.760500</td>\n","    </tr>\n","    <tr>\n","      <td>1180</td>\n","      <td>1.579800</td>\n","    </tr>\n","    <tr>\n","      <td>1185</td>\n","      <td>1.711300</td>\n","    </tr>\n","    <tr>\n","      <td>1190</td>\n","      <td>1.704200</td>\n","    </tr>\n","    <tr>\n","      <td>1195</td>\n","      <td>1.554200</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>1.464500</td>\n","    </tr>\n","    <tr>\n","      <td>1205</td>\n","      <td>1.512700</td>\n","    </tr>\n","    <tr>\n","      <td>1210</td>\n","      <td>1.529900</td>\n","    </tr>\n","    <tr>\n","      <td>1215</td>\n","      <td>1.687700</td>\n","    </tr>\n","    <tr>\n","      <td>1220</td>\n","      <td>1.645300</td>\n","    </tr>\n","    <tr>\n","      <td>1225</td>\n","      <td>1.832900</td>\n","    </tr>\n","    <tr>\n","      <td>1230</td>\n","      <td>1.739600</td>\n","    </tr>\n","    <tr>\n","      <td>1235</td>\n","      <td>1.541500</td>\n","    </tr>\n","    <tr>\n","      <td>1240</td>\n","      <td>1.572700</td>\n","    </tr>\n","    <tr>\n","      <td>1245</td>\n","      <td>1.672700</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>1.650600</td>\n","    </tr>\n","    <tr>\n","      <td>1255</td>\n","      <td>1.494900</td>\n","    </tr>\n","    <tr>\n","      <td>1260</td>\n","      <td>1.366900</td>\n","    </tr>\n","    <tr>\n","      <td>1265</td>\n","      <td>1.660100</td>\n","    </tr>\n","    <tr>\n","      <td>1270</td>\n","      <td>1.501500</td>\n","    </tr>\n","    <tr>\n","      <td>1275</td>\n","      <td>1.582300</td>\n","    </tr>\n","    <tr>\n","      <td>1280</td>\n","      <td>1.602800</td>\n","    </tr>\n","    <tr>\n","      <td>1285</td>\n","      <td>1.667000</td>\n","    </tr>\n","    <tr>\n","      <td>1290</td>\n","      <td>1.346400</td>\n","    </tr>\n","    <tr>\n","      <td>1295</td>\n","      <td>1.475300</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>1.641500</td>\n","    </tr>\n","    <tr>\n","      <td>1305</td>\n","      <td>1.383600</td>\n","    </tr>\n","    <tr>\n","      <td>1310</td>\n","      <td>1.403400</td>\n","    </tr>\n","    <tr>\n","      <td>1315</td>\n","      <td>1.465700</td>\n","    </tr>\n","    <tr>\n","      <td>1320</td>\n","      <td>1.779100</td>\n","    </tr>\n","    <tr>\n","      <td>1325</td>\n","      <td>1.659200</td>\n","    </tr>\n","    <tr>\n","      <td>1330</td>\n","      <td>1.729300</td>\n","    </tr>\n","    <tr>\n","      <td>1335</td>\n","      <td>1.675900</td>\n","    </tr>\n","    <tr>\n","      <td>1340</td>\n","      <td>1.817800</td>\n","    </tr>\n","    <tr>\n","      <td>1345</td>\n","      <td>1.714800</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>1.523800</td>\n","    </tr>\n","    <tr>\n","      <td>1355</td>\n","      <td>1.587400</td>\n","    </tr>\n","    <tr>\n","      <td>1360</td>\n","      <td>1.628600</td>\n","    </tr>\n","    <tr>\n","      <td>1365</td>\n","      <td>1.659400</td>\n","    </tr>\n","    <tr>\n","      <td>1370</td>\n","      <td>1.741500</td>\n","    </tr>\n","    <tr>\n","      <td>1375</td>\n","      <td>1.655600</td>\n","    </tr>\n","    <tr>\n","      <td>1380</td>\n","      <td>1.629500</td>\n","    </tr>\n","    <tr>\n","      <td>1385</td>\n","      <td>1.521000</td>\n","    </tr>\n","    <tr>\n","      <td>1390</td>\n","      <td>1.522800</td>\n","    </tr>\n","    <tr>\n","      <td>1395</td>\n","      <td>1.679800</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>1.418800</td>\n","    </tr>\n","    <tr>\n","      <td>1405</td>\n","      <td>1.549200</td>\n","    </tr>\n","    <tr>\n","      <td>1410</td>\n","      <td>1.600900</td>\n","    </tr>\n","    <tr>\n","      <td>1415</td>\n","      <td>1.458100</td>\n","    </tr>\n","    <tr>\n","      <td>1420</td>\n","      <td>1.601600</td>\n","    </tr>\n","    <tr>\n","      <td>1425</td>\n","      <td>1.590100</td>\n","    </tr>\n","    <tr>\n","      <td>1430</td>\n","      <td>1.498000</td>\n","    </tr>\n","    <tr>\n","      <td>1435</td>\n","      <td>1.649300</td>\n","    </tr>\n","    <tr>\n","      <td>1440</td>\n","      <td>1.772700</td>\n","    </tr>\n","    <tr>\n","      <td>1445</td>\n","      <td>1.528500</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>1.513000</td>\n","    </tr>\n","    <tr>\n","      <td>1455</td>\n","      <td>1.635500</td>\n","    </tr>\n","    <tr>\n","      <td>1460</td>\n","      <td>1.552600</td>\n","    </tr>\n","    <tr>\n","      <td>1465</td>\n","      <td>1.701000</td>\n","    </tr>\n","    <tr>\n","      <td>1470</td>\n","      <td>1.537900</td>\n","    </tr>\n","    <tr>\n","      <td>1475</td>\n","      <td>1.607700</td>\n","    </tr>\n","    <tr>\n","      <td>1480</td>\n","      <td>1.622100</td>\n","    </tr>\n","    <tr>\n","      <td>1485</td>\n","      <td>1.651400</td>\n","    </tr>\n","    <tr>\n","      <td>1490</td>\n","      <td>1.358000</td>\n","    </tr>\n","    <tr>\n","      <td>1495</td>\n","      <td>1.624400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.520100</td>\n","    </tr>\n","    <tr>\n","      <td>1505</td>\n","      <td>1.597500</td>\n","    </tr>\n","    <tr>\n","      <td>1510</td>\n","      <td>1.666900</td>\n","    </tr>\n","    <tr>\n","      <td>1515</td>\n","      <td>1.877600</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>1.599400</td>\n","    </tr>\n","    <tr>\n","      <td>1525</td>\n","      <td>1.587200</td>\n","    </tr>\n","    <tr>\n","      <td>1530</td>\n","      <td>1.578200</td>\n","    </tr>\n","    <tr>\n","      <td>1535</td>\n","      <td>1.403100</td>\n","    </tr>\n","    <tr>\n","      <td>1540</td>\n","      <td>1.716400</td>\n","    </tr>\n","    <tr>\n","      <td>1545</td>\n","      <td>1.611800</td>\n","    </tr>\n","    <tr>\n","      <td>1550</td>\n","      <td>1.575800</td>\n","    </tr>\n","    <tr>\n","      <td>1555</td>\n","      <td>1.495100</td>\n","    </tr>\n","    <tr>\n","      <td>1560</td>\n","      <td>1.711100</td>\n","    </tr>\n","    <tr>\n","      <td>1565</td>\n","      <td>1.573400</td>\n","    </tr>\n","    <tr>\n","      <td>1570</td>\n","      <td>1.842500</td>\n","    </tr>\n","    <tr>\n","      <td>1575</td>\n","      <td>1.470300</td>\n","    </tr>\n","    <tr>\n","      <td>1580</td>\n","      <td>1.533700</td>\n","    </tr>\n","    <tr>\n","      <td>1585</td>\n","      <td>1.502200</td>\n","    </tr>\n","    <tr>\n","      <td>1590</td>\n","      <td>1.343500</td>\n","    </tr>\n","    <tr>\n","      <td>1595</td>\n","      <td>1.523300</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>1.489500</td>\n","    </tr>\n","    <tr>\n","      <td>1605</td>\n","      <td>1.402100</td>\n","    </tr>\n","    <tr>\n","      <td>1610</td>\n","      <td>1.579300</td>\n","    </tr>\n","    <tr>\n","      <td>1615</td>\n","      <td>1.508200</td>\n","    </tr>\n","    <tr>\n","      <td>1620</td>\n","      <td>1.423800</td>\n","    </tr>\n","    <tr>\n","      <td>1625</td>\n","      <td>1.668800</td>\n","    </tr>\n","    <tr>\n","      <td>1630</td>\n","      <td>1.577900</td>\n","    </tr>\n","    <tr>\n","      <td>1635</td>\n","      <td>1.837800</td>\n","    </tr>\n","    <tr>\n","      <td>1640</td>\n","      <td>1.598300</td>\n","    </tr>\n","    <tr>\n","      <td>1645</td>\n","      <td>1.566200</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>1.501000</td>\n","    </tr>\n","    <tr>\n","      <td>1655</td>\n","      <td>1.475000</td>\n","    </tr>\n","    <tr>\n","      <td>1660</td>\n","      <td>1.398600</td>\n","    </tr>\n","    <tr>\n","      <td>1665</td>\n","      <td>1.281200</td>\n","    </tr>\n","    <tr>\n","      <td>1670</td>\n","      <td>1.687200</td>\n","    </tr>\n","    <tr>\n","      <td>1675</td>\n","      <td>1.470100</td>\n","    </tr>\n","    <tr>\n","      <td>1680</td>\n","      <td>1.528400</td>\n","    </tr>\n","    <tr>\n","      <td>1685</td>\n","      <td>1.440800</td>\n","    </tr>\n","    <tr>\n","      <td>1690</td>\n","      <td>1.513400</td>\n","    </tr>\n","    <tr>\n","      <td>1695</td>\n","      <td>1.589400</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>1.467200</td>\n","    </tr>\n","    <tr>\n","      <td>1705</td>\n","      <td>1.501200</td>\n","    </tr>\n","    <tr>\n","      <td>1710</td>\n","      <td>1.474600</td>\n","    </tr>\n","    <tr>\n","      <td>1715</td>\n","      <td>1.412300</td>\n","    </tr>\n","    <tr>\n","      <td>1720</td>\n","      <td>1.608000</td>\n","    </tr>\n","    <tr>\n","      <td>1725</td>\n","      <td>1.446300</td>\n","    </tr>\n","    <tr>\n","      <td>1730</td>\n","      <td>1.645900</td>\n","    </tr>\n","    <tr>\n","      <td>1735</td>\n","      <td>1.448100</td>\n","    </tr>\n","    <tr>\n","      <td>1740</td>\n","      <td>1.722100</td>\n","    </tr>\n","    <tr>\n","      <td>1745</td>\n","      <td>1.511000</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>1.463800</td>\n","    </tr>\n","    <tr>\n","      <td>1755</td>\n","      <td>1.682900</td>\n","    </tr>\n","    <tr>\n","      <td>1760</td>\n","      <td>1.547600</td>\n","    </tr>\n","    <tr>\n","      <td>1765</td>\n","      <td>1.541900</td>\n","    </tr>\n","    <tr>\n","      <td>1770</td>\n","      <td>1.478800</td>\n","    </tr>\n","    <tr>\n","      <td>1775</td>\n","      <td>1.512200</td>\n","    </tr>\n","    <tr>\n","      <td>1780</td>\n","      <td>1.492700</td>\n","    </tr>\n","    <tr>\n","      <td>1785</td>\n","      <td>1.659200</td>\n","    </tr>\n","    <tr>\n","      <td>1790</td>\n","      <td>1.488400</td>\n","    </tr>\n","    <tr>\n","      <td>1795</td>\n","      <td>1.628700</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>1.410900</td>\n","    </tr>\n","    <tr>\n","      <td>1805</td>\n","      <td>1.585100</td>\n","    </tr>\n","    <tr>\n","      <td>1810</td>\n","      <td>1.463800</td>\n","    </tr>\n","    <tr>\n","      <td>1815</td>\n","      <td>1.553600</td>\n","    </tr>\n","    <tr>\n","      <td>1820</td>\n","      <td>1.504600</td>\n","    </tr>\n","    <tr>\n","      <td>1825</td>\n","      <td>1.436000</td>\n","    </tr>\n","    <tr>\n","      <td>1830</td>\n","      <td>1.770100</td>\n","    </tr>\n","    <tr>\n","      <td>1835</td>\n","      <td>1.791500</td>\n","    </tr>\n","    <tr>\n","      <td>1840</td>\n","      <td>1.445300</td>\n","    </tr>\n","    <tr>\n","      <td>1845</td>\n","      <td>1.505500</td>\n","    </tr>\n","    <tr>\n","      <td>1850</td>\n","      <td>1.554100</td>\n","    </tr>\n","    <tr>\n","      <td>1855</td>\n","      <td>1.483900</td>\n","    </tr>\n","    <tr>\n","      <td>1860</td>\n","      <td>1.709300</td>\n","    </tr>\n","    <tr>\n","      <td>1865</td>\n","      <td>1.525500</td>\n","    </tr>\n","    <tr>\n","      <td>1870</td>\n","      <td>1.505000</td>\n","    </tr>\n","    <tr>\n","      <td>1875</td>\n","      <td>1.617000</td>\n","    </tr>\n","    <tr>\n","      <td>1880</td>\n","      <td>1.717700</td>\n","    </tr>\n","    <tr>\n","      <td>1885</td>\n","      <td>1.606300</td>\n","    </tr>\n","    <tr>\n","      <td>1890</td>\n","      <td>1.397100</td>\n","    </tr>\n","    <tr>\n","      <td>1895</td>\n","      <td>1.462600</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>1.812500</td>\n","    </tr>\n","    <tr>\n","      <td>1905</td>\n","      <td>1.504700</td>\n","    </tr>\n","    <tr>\n","      <td>1910</td>\n","      <td>1.666300</td>\n","    </tr>\n","    <tr>\n","      <td>1915</td>\n","      <td>1.541300</td>\n","    </tr>\n","    <tr>\n","      <td>1920</td>\n","      <td>1.488100</td>\n","    </tr>\n","    <tr>\n","      <td>1925</td>\n","      <td>1.258200</td>\n","    </tr>\n","    <tr>\n","      <td>1930</td>\n","      <td>1.675100</td>\n","    </tr>\n","    <tr>\n","      <td>1935</td>\n","      <td>1.693600</td>\n","    </tr>\n","    <tr>\n","      <td>1940</td>\n","      <td>1.553700</td>\n","    </tr>\n","    <tr>\n","      <td>1945</td>\n","      <td>1.691600</td>\n","    </tr>\n","    <tr>\n","      <td>1950</td>\n","      <td>1.397600</td>\n","    </tr>\n","    <tr>\n","      <td>1955</td>\n","      <td>1.634200</td>\n","    </tr>\n","    <tr>\n","      <td>1960</td>\n","      <td>1.352400</td>\n","    </tr>\n","    <tr>\n","      <td>1965</td>\n","      <td>1.478200</td>\n","    </tr>\n","    <tr>\n","      <td>1970</td>\n","      <td>1.519400</td>\n","    </tr>\n","    <tr>\n","      <td>1975</td>\n","      <td>1.640500</td>\n","    </tr>\n","    <tr>\n","      <td>1980</td>\n","      <td>1.648700</td>\n","    </tr>\n","    <tr>\n","      <td>1985</td>\n","      <td>1.484100</td>\n","    </tr>\n","    <tr>\n","      <td>1990</td>\n","      <td>1.425400</td>\n","    </tr>\n","    <tr>\n","      <td>1995</td>\n","      <td>1.574900</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.593700</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>1.587200</td>\n","    </tr>\n","    <tr>\n","      <td>2010</td>\n","      <td>1.541100</td>\n","    </tr>\n","    <tr>\n","      <td>2015</td>\n","      <td>1.463000</td>\n","    </tr>\n","    <tr>\n","      <td>2020</td>\n","      <td>1.408900</td>\n","    </tr>\n","    <tr>\n","      <td>2025</td>\n","      <td>1.485600</td>\n","    </tr>\n","    <tr>\n","      <td>2030</td>\n","      <td>1.499800</td>\n","    </tr>\n","    <tr>\n","      <td>2035</td>\n","      <td>1.535800</td>\n","    </tr>\n","    <tr>\n","      <td>2040</td>\n","      <td>1.445500</td>\n","    </tr>\n","    <tr>\n","      <td>2045</td>\n","      <td>1.665700</td>\n","    </tr>\n","    <tr>\n","      <td>2050</td>\n","      <td>1.623800</td>\n","    </tr>\n","    <tr>\n","      <td>2055</td>\n","      <td>1.358900</td>\n","    </tr>\n","    <tr>\n","      <td>2060</td>\n","      <td>1.340300</td>\n","    </tr>\n","    <tr>\n","      <td>2065</td>\n","      <td>1.526100</td>\n","    </tr>\n","    <tr>\n","      <td>2070</td>\n","      <td>1.456200</td>\n","    </tr>\n","    <tr>\n","      <td>2075</td>\n","      <td>1.454800</td>\n","    </tr>\n","    <tr>\n","      <td>2080</td>\n","      <td>1.377000</td>\n","    </tr>\n","    <tr>\n","      <td>2085</td>\n","      <td>1.636400</td>\n","    </tr>\n","    <tr>\n","      <td>2090</td>\n","      <td>1.352400</td>\n","    </tr>\n","    <tr>\n","      <td>2095</td>\n","      <td>1.559400</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>1.652000</td>\n","    </tr>\n","    <tr>\n","      <td>2105</td>\n","      <td>1.439500</td>\n","    </tr>\n","    <tr>\n","      <td>2110</td>\n","      <td>1.555900</td>\n","    </tr>\n","    <tr>\n","      <td>2115</td>\n","      <td>1.367400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=2119, training_loss=1.6154496970183447, metrics={'train_runtime': 4971.0668, 'train_samples_per_second': 1.706, 'train_steps_per_second': 0.426, 'total_flos': 1.965807713334067e+16, 'train_loss': 1.6154496970183447, 'epoch': 1.0})"]},"metadata":{},"execution_count":11}],"source":["trainer.train()"]},{"cell_type":"markdown","source":["# After training"],"metadata":{"id":"w9PoICeL441L"}},{"cell_type":"code","source":["def generate_example(gen_model, tokenizer, dataset, num_examples=5):\n","    for i in range(num_examples):\n","        example_input = dataset[\"test\"][i][\"input_ids\"].to(\"cuda\")\n","        example_label = dataset[\"test\"][i][\"labels\"].to(\"cuda\")\n","\n","        output = gen_model.generate(\n","            input_ids=example_input.unsqueeze(0),\n","            max_new_tokens=1000,\n","            temperature=0.9,\n","            top_p=0.9,\n","            num_return_sequences=1,\n","            #repetition_penalty=100.0\n","\n","        )\n","\n","        print(\"RESULT: \", i)\n","        print(tokenizer.decode(output[0], skip_special_tokens=True))\n","        # decode input and label\n","        print(\"INPUT: \", i)\n","        print(tokenizer.decode(example_input, skip_special_tokens=True))\n","        print(\"LABEL: \", i)\n","        print(tokenizer.decode(example_label, skip_special_tokens=True))\n","        print(\"--------------------------------------------------------\")\n","\n","generate_example(model, tokenizer, dataset, num_examples=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3DvBlF243iQ","executionInfo":{"status":"ok","timestamp":1686477605699,"user_tz":-120,"elapsed":96947,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}},"outputId":"b2defad0-8d07-45d0-eae7-0b8fb7b7374a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["RESULT:  0\n","The correct answer is: All of the above.\n","INPUT:  0\n","Question: What standard minimal information about a dataset is needed in order to understand, interpret and use it? • Subject, methods and brain location • Type and format of data • Contributors, license and access to the data • All of the above Answer: \n","LABEL:  0\n","The correct answer is: All of the above. Subject, methods, and brain location provide context and understanding of the data. The type and format of data determine what analyses can be performed on the data. Contributors, license, and access to the data determine if the data can be legally and ethically used. Therefore, all of the above are needed to understand, interpret, and use a dataset.\n","--------------------------------------------------------\n","RESULT:  1\n","The correct answer is A. when a set of variables in the feature space are not linearly independent. This is because feature degeneracy is a problem in the field of feature analysis. The correct answer is A. when a set of variables in the feature space are not linearly independent. This is because feature degeneracy is a problem in the field of feature analysis.\n","INPUT:  1\n","Question: Please answer the following question by selecting one correct choice and provide a simple explanation for your selection. Description: Feature degeneracy: Options: A. when a set of variables in the feature space are not linearly independent B. when a set of variables in the feature space create chaotic results C. when a set of variables in the feature space have low impact on the output space D. when a point in output space can be obtained by one and only one set of variables in the feature space. Let's think step by step in mind, but not necessarily write it all down. Answer: \n","LABEL:  1\n","The correct answer is A. when a set of variables in the feature space are not linearly independent. Explanation: Feature degeneracy refers to a situation where a set of variables in the feature space are not linearly independent. In other words, one or more variables in the set can be expressed as linear combinations of the others. This can lead to problems in machine learning models, as the models may not be able to distinguish between the different variables or may assign them too much or too little importance. For example, suppose we have two variables in the feature space: height in feet and height in inches. These two variables are not linearly independent, as one can be expressed as a linear combination of the other (1 foot = 12 inches). Therefore, using both of these variables in a machine learning model may lead to feature degeneracy, as the model may not be able to distinguish between the two variables or may assign them too much or too little importance. Therefore, the correct answer is A.\n","--------------------------------------------------------\n","RESULT:  2\n","No, there are no other options that answer the question above.\n","INPUT:  2\n","Question: What is polarization? Answer: Polarization refers to the direction of oscillation of electromagnetic waves, such as light waves. When light waves are polarized, their oscillations occur in a specific direction. Polarization can be linear, circular, or elliptical, depending on the orientation of the oscillations. In biomedical imaging, polarization can be used to enhance contrast and improve image quality by selectively filtering out unwanted light waves. For example, polarized light microscopy is used in histology to visualize the orientation of collagen fibers in tissues, which can provide important insights into their structure and function. Question: From the following options, which option would you choose as the answer to the question above: A. Defined direction of oscillation for transverse waves B. Defined direction of oscillation for longitudinal waves C. Perpendicular directions of oscillations for E & B fields D. Inherent property of light Answer: A. Defined direction of oscillation for transverse waves. Question: There might be more than one correct options, is there any other options that also answer the question above? Answer: \n","LABEL:  2\n","Yes, another correct option could be: D. Inherent property of light\n","--------------------------------------------------------\n","RESULT:  3\n","Consistent activity in specific brain regions is a key factor in the creation of functional maps. Consistent activity in specific brain regions can help to identify regions that are most likely to be involved in a specific cognitive process. Consistent activity in specific brain regions can also help to identify regions that are not being used in a specific cognitive task, such as those that are not being used in a specific cognitive task. Consistent activity in specific brain regions can also help to identify regions that are not being used in a specific cognitive task, such as those that are not being used in a specific cognitive task. Consistent activity in specific brain regions can also help to identify regions that are not being used in a specific cognitive task, such as those that are not being used in a specific cognitive task. Consistent activity in specific brain regions can also help to identify regions that are not being used in a specific cognitive task, such as those that are not being used in a specific cognitive task. Consistent activity in specific brain regions can also help to identify regions that are not being used in a specific cognitive task, such as those that are not being used in a specific cognitive task.\n","INPUT:  3\n","Question: MCQ: How were functional maps valid for reverse inference obtained? Choices: 1) By looking across several cognitive labels for the group of regions that most often reflect a specific cognitive process happening 2) By combining clustering and matrix decomposition to assign a region to a cognitive process 3) by using linear autoencoders to discover the structures underlying a specific cognitive process 4) No such maps could be obtained at this time Answer: Answer: 1) By looking across several cognitive labels for the group of regions that most often reflect a specific cognitive process happening Explanation: Functional maps valid for reverse inference were obtained by looking across several cognitive labels for the group of regions that most often reflect a specific cognitive process happening. These maps were derived from meta-analyses of brain imaging studies that associated specific tasks with specific brain regions. The meta-analyses were used to identify the most consistently activated regions across studies that used the same cognitive task. These regions were then labeled with the cognitive process that was most often associated with their activation, and the resulting maps were used for reverse inference. Question: What role did consistent activity in specific brain regions play in the creation of functional maps? Answer: \n","LABEL:  3\n","Consistent activity in specific brain regions played a crucial role in the creation of functional maps. The maps were derived from meta-analyses of brain imaging studies that associated specific tasks with specific brain regions. The meta-analyses were used to identify the most consistently activated regions across studies that used the same cognitive task. These consistently activated regions were then labeled with the cognitive process that was most often associated with their activation, and the resulting maps were used for reverse inference. Therefore, consistent activity in specific brain regions was necessary to identify the regions that are consistently involved in a specific cognitive process and to create functional maps.\n","--------------------------------------------------------\n","RESULT:  4\n","Tout d'abord, nous doivent utiliser la fonction d'élimination d'un matrice sym'étrique de taille $ntimes n$ en utilisant la fonction d'élimination de $A$ et $lambda_1ne lambda_2$. En utilisant la fonction d'élimination de $A$, il est possible de d'écrire : $$lambda_1ne lambda_2lambda_1ne lambda_2lambda_1ne lambda_1ne lambda_2lambda_1ne lambda_1ne lambda_1ne lambda_2lambda_2lambda_1ne lambda_2lambda_1ne lambda_2lambda_2neqvec v_1cdot v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec v_2vec\n","INPUT:  4\n","Question: Soit $A$ une matrice sym'etrique de taille $ntimes n$, $lambda_1, lambda_2$ deux valeurs propres de $A$, et $vec v_1neqvec 0,vec v_2neqvec 0$ des vecteurs propres associ'es.Sans utiliser le th'eoreme spectral, montrer que si $lambda_1ne lambda_2$, alors $vec v_1cdot vec v_2 = 0$. Answer: \n","LABEL:  4\n","Avant de commencer, rappelons que pour tout vecteur propre $vec v$ associ'e a une valeur propre $lambda$ de $A$, on a $Avec v=lambdavec v$. Supposons que $lambda_1$ et $lambda_2$ sont deux valeurs propres distinctes de $A$, et $vec v_1$ et $vec v_2$ sont des vecteurs propres associ'es non nuls. On veut montrer que $vec v_1cdot vec v_2 = 0$. Pour cela, rappelons que le produit scalaire de deux vecteurs $vec u$ et $vec w$ est donn'e par $vec ucdot vec w = sum_i=1n u_iw_i$. Ainsi, le produit scalaire de $vec v_1$ et $vec v_2$ est donn'e par $vec v_1cdot vec v_2 = sum_i=1n v_1,iv_2,i$. Maintenant, utilisons le fait que $A$ est une matrice sym'etrique, c'est-a-dire que $A=AT$. On a alors : beginalign* vec v_1cdot Avec v_2 &= vec v_1cdot (Avec v_2) &= vec v_1cdot (lambda_2vec v_2) qquad text(car $vec v_2$ est un\n","--------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["# Checkpoint 1"],"metadata":{"id":"IpD2e4cl7xvX"}},{"cell_type":"code","source":["from peft import PeftModel, PeftConfig\n","peft_model_id = \"../../../outputs/checkpoint-632\"\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, device_map=\"auto\")\n","model1 = PeftModel.from_pretrained(model, peft_model_id)\n","model1.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7seWdk1m8z8E","outputId":"c9081343-e7a9-47bb-93ab-d189392ed6cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PeftModelForSeq2SeqLM(\n","  (base_model): LoraModel(\n","    (model): T5ForConditionalGeneration(\n","      (shared): Embedding(32128, 768)\n","      (encoder): T5Stack(\n","        (embed_tokens): Embedding(32128, 768)\n","        (block): ModuleList(\n","          (0): T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                  (relative_attention_bias): Embedding(32, 12)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","          (1-11): 11 x T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","        (final_layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (decoder): T5Stack(\n","        (embed_tokens): Embedding(32128, 768)\n","        (block): ModuleList(\n","          (0): T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                  (relative_attention_bias): Embedding(32, 12)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerCrossAttention(\n","                (EncDecAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (2): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","          (1-11): 11 x T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerCrossAttention(\n","                (EncDecAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=16, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=16, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (2): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","        (final_layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FTUWwKyXAxc6","outputId":"5351af98-6ba0-40c4-b8ee-7f09f7afe68f"},"outputs":[{"output_type":"stream","name":"stdout","text":["RESULT:  0\n","The correct answer is (A) Because of the cosmological shift towards smaller wavelengths at high redshift, one has to observe the H alpha emission line in an ultraviolet filter. This means that there are no visible signs of stellar formation in very old galaxie with the redshift of about 5. Therefore, we need to use the R filter covering the wavelength range from 6000–7000 Angström.\n","INPUT:  0\n","Question: We look for signs of the stellar formation in very old galaxies with the redshift of about 5. So one wants to observe the H alpha emission line in the gas clouds made of ionised hydrogen. The R filter (covering the wavelength range from 6000 to 7000 Angström) seems to be the right filter to use. Unfortunately, there is nothing in the images. Why? (A) Because of the cosmological shift towards smaller wavelengths at high redshift, one has to observe the H alpha line in an ultraviolet filter. (B) Because of the shift towards longer wavelengths at high redshift, one has to observe the H alpha line in an infrared filter. (C) Probably the R filter is dirty, one has to clean it. (D) The galaxies at very high redshift are not bright enough to be observed. Answer: Let’s think step by step: Answer: \n","LABEL:  0\n","At high redshift, there is a cosmological shift towards longer wavelengths, meaning that the H alpha emission line will be shifted towards longer wavelengths. Therefore, (A) is not correct. It is also not likely that the filter is dirty, so (C) can be ruled out. The correct answer is (B), because the shift towards longer wavelengths means that the H alpha line will be shifted towards the infrared part of the spectrum, and therefore one needs an infrared filter to observe it. Finally, (D) is not necessarily correct, as there are ways to observe faint galaxies even at high redshifts, such as using longer exposure times or larger telescopes.\n","--------------------------------------------------------\n","RESULT:  1\n","Bien sûr! Tout d'abord, rappelons que $T_A:Vto V$ est définie par $T_A(M)=AM-MA$. En effet, la fonction de calculer le système des valeurs du produit pour les deux variables sont donc déterminées dans un autre cas suivant l'application Linéaire T_A (m). Pour cela, on doit utiliser une méthode donnée comme suit: La formule \"a\" = A + B = C$ qui permet de déterminer si $A$ soit inégale au sein de ce processus ou non seulement sur le modèle M/B. On peut également vérifier qu'il existe trois dimensions permettant de trouver une solution différentielle entre ces two conditions; alors nous pouvions maintenant montrer que $C$ représente une dimension distinctielle plus large que celle dit après avoir été trouvée.\n","INPUT:  1\n","Question: Peut-tu mieux expliquer ton raisonnement? Answer: Bien sûr! Tout d'abord, rappelons que l'application linéaire $T_A:Vto V$ est définie par $T_A(M)=AM-MA$, où $A$ est la matrice donnée dans l'énoncé et $V$ est le sous-espace vectoriel de $M_2times 2(mathbbR)$ constitué des matrices symétriques. Ensuite, on nous demande de calculer l'image d'une matrice symétrique générique de $V$ sous l'action de $T_A$. Cette matrice est de la forme $beginpmatrix a & b  b & c endpmatrix$, où $a,b,c$ sont des réels quelconques. Ainsi, on calcule: $$ beginaligned T_Aleft( beginpmatrix a & b  b & c endpmatrixright) &= A beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix A  &= beginpmatrix -2 & 4 & 0  0 & 3 & 0  1 & -2 & 1 endpmatrix beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix beg\n","LABEL:  1\n","Bien sûr, voici la réponse sous forme de LaTeX: La réponse correcte est: $$T_Aleft( beginpmatrix a & b  b & c endpmatrixright) = beginpmatrix -2a+4b & a-2b+c  a-2b+c & 3c endpmatrix.$$\n","--------------------------------------------------------\n","RESULT:  2\n","Pour déterminer si (f',f'':mathbbRtomathbm$ est continue sur (mathbubbleR), nous devons utiliser la formule suivante pour calculer les deux fonctions dérivées première et second. On peut donc trouver le résultat des trois lignes dans une colonne donné par un coefficient supérieur à celui du point inférieur qui existe au niveau élevé ou plus haut que celle entre ces two limites (c'est-à-dire) avec respect aux valeurs propres). En effet, on obtient $F_0=1+2x_-1 = 1 + 2x_-2 = 3x_-3 = 4x_-4x_-5x_-6x_-7yx_-8x_-9x_-10x_-11x_-13x_-12x_-14x_-15x_-16x_-17x_-18x_-21x_-22x_-24x_-25x_-60x_-21x_-23x_0x_0x_0x_0-27x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0\n","INPUT:  2\n","Question: Soit (f:mathbbRtomathbbR) une fonction deux fois dérivable sur (mathbbR) et (f',f'':mathbbRtomathbbR) ses fonctions dérivées première et deuxième. Alors 1) (f) est continue sur (mathbbR) 2) (f') est continue sur (mathbbR) 3) (f'') est continue sur (mathbbR) 4) pour tout (x_0inmathbbR) (displaystylelim_xto x_0f'(x)) existe 5) pour tout (x_0inmathbbR) (displaystylelim_xto x_0fracf'(x)-f'(x_0)x-x_0) existe Answer: \n","LABEL:  2\n","Tout d'abord, nous savons que la dérivabilité de (f) implique sa continuité. Donc, la proposition 1 est vraie car (f) est deux fois dérivable, donc elle est aussi dérivable et donc continue. Ensuite, la proposition 2 est vraie car si (f) est deux fois dérivable, alors sa dérivée première (f') est dérivable. Et nous savons que la dérivabilité implique la continuité. La proposition 3 n'est pas nécessairement vraie. Le fait que (f'') existe ne garantit pas sa continuité. Par exemple, la fonction (f(x)=|x|) est deux fois dérivable sur (mathbbR), mais sa dérivée seconde n'est pas continue en (x=0). La proposition 4 est vraie car si (f') est dérivable en (x_0), alors elle est continue en (x_0). Enfin, la proposition 5 est vraie car si (f'') existe en (x_0), alors la limite (displaystylelim_xto x_0fracf'(x)-f'(x_0)x-x_0) existe et est égale à (f''(x_0)). Cela correspond à la définition de la dérivée seconde de (f) en (x_0).\n","--------------------------------------------------------\n","RESULT:  3\n","To solve this problem, we need to use the formula for [mathjaxinline]alpha=1, beta=0.1, [/mathjaxinline]. In this case, [mathji = 1] and [mathdi = 2], where [mathcm = 0] and [mathdi = 3] are constants. We can find that in both cases, [mathbq = -1] and [mathdi = 4] are not equal. Therefore, the correct answer is: [math_langle nrangle=left.fracpartial Gpartial zright|_z=1=[/mathjoxinline].\n","INPUT:  3\n","Question: In the case where [mathjaxinline]alpha=1, beta=0.1, [/mathjaxinline][mathjaxinline]langle nrangle=left.fracpartial Gpartial zright|_z=1=[/mathjaxinline] Answer: \n","LABEL:  3\n","Assuming that [mathjaxinline]G(z)[/mathjaxinline] is the generating function for the sequence [mathjaxinline]n_k[/mathjaxinline], where [mathjaxinline]n_k[/mathjaxinline] denotes the number of occurrences of some event in the k-th trial, we have: [mathjaxinline]G(z) = sum_k=0infty n_k zk[/mathjaxinline] Given that [mathjaxinline]langle nrangle=left.fracpartial Gpartial zright|_z=1[/mathjaxinline], we can differentiate [mathjaxinline]G(z)[/mathjaxinline] with respect to [mathjaxinline]z[/mathjaxinline] and then set [mathjaxinline]z=1[/mathjaxinline] to obtain: [mathjaxinline]langle nrangle = left.fracpartial Gpartial zright|_z=1 = sum_k=1infty k n_k[/mathjaxinline] Now, using the fact that [mathjaxinline]alpha=1[/mathjaxinline] and [mathjaxinline]beta=0.1[/mathjaxinline], we can write: [mathjaxinline]n_k = frac(k-1)!(1-alpha-beta)k!alphakbeta1-1 = frac110kalphak[/mathjaxinline] Substituting this expression for [mathjaxinline]n_k[/mathjaxinline] into the equation for [mathjaxinline]langle nrangle[/mathjaxinline], we get: [\n","--------------------------------------------------------\n","RESULT:  4\n","The cluster assignments in a k-means algorithm are the number of steps that an individual can perform to achieve a goal. This is because each step requires a specific amount of time and effort, which is not necessarily necessary for the task being performed. Therefore, the correct answer is H.\n","INPUT:  4\n","Question: Classify the following quantity as parameters (P) or hyperparameters (H) of a learning algorithm: The cluster assignments in a k-means algorithm. Answer: \n","LABEL:  4\n","In a k-means algorithm, the cluster assignments are parameters (P) rather than hyperparameters (H). Here's why: - Parameters are variables that are learned during the training process. In the case of k-means, the cluster assignments (i.e., which data points belong to which cluster) are determined during the training process and are updated iteratively until convergence. Therefore, they are considered parameters of the algorithm. - Hyperparameters, on the other hand, are values that are set before the training process begins and are not learned during training. Examples of hyperparameters in k-means include the number of clusters (k), the initialization method, and the convergence criteria. These hyperparameters are set before the training process begins and are not updated during training. So, in summary, the cluster assignments in a k-means algorithm are parameters (P) of the algorithm rather than hyperparameters (H).\n","--------------------------------------------------------\n"]}],"source":["def generate_example(gen_model, tokenizer, dataset, num_examples=5):\n","    for i in range(num_examples):\n","        example_input = dataset[\"test\"][i][\"input_ids\"].to(\"cuda\")\n","        example_label = dataset[\"test\"][i][\"labels\"].to(\"cuda\")\n","\n","        output = gen_model.generate(\n","            input_ids=example_input.unsqueeze(0),\n","            max_new_tokens=1000,\n","            temperature=0.9,\n","            top_p=0.9,\n","            num_return_sequences=1,\n","            repetition_penalty=100.0\n","\n","        )\n","\n","        print(\"RESULT: \", i)\n","        print(tokenizer.decode(output[0], skip_special_tokens=True))\n","        # decode input and label\n","        print(\"INPUT: \", i)\n","        print(tokenizer.decode(example_input, skip_special_tokens=True))\n","        print(\"LABEL: \", i)\n","        print(tokenizer.decode(example_label, skip_special_tokens=True))\n","        print(\"--------------------------------------------------------\")\n","\n","generate_example(model1, tokenizer, dataset, num_examples=5)"]},{"cell_type":"markdown","source":["# checkpoint 4"],"metadata":{"id":"89JmlbXY-yl7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xa-im4bKv8Fh"},"outputs":[],"source":["from peft import PeftModel, PeftConfig\n","peft_model_id = \"../../trained_peft\"\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, device_map=\"auto\")\n","model4 = PeftModel.from_pretrained(model, peft_model_id)\n","model4 = model4.eval()"]},{"cell_type":"code","source":["generate_example(model4, tokenizer, dataset, num_examples=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YX3b1Dl8-6-j","outputId":"b2c84b5d-d281-44f0-fe45-4d81ed8b1b53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RESULT:  0\n","The correct answer is (A) Because of the cosmological shift towards smaller wavelengths at high redshift, one has to observe the H alpha emission line in an ultraviolet filter. This means that there are no visible signs of stellar formation in very old galaxie with the redshift of about 5. Therefore, we need to use the R filter covering the wavelength range from 6000–7000 Angström.\n","INPUT:  0\n","Question: We look for signs of the stellar formation in very old galaxies with the redshift of about 5. So one wants to observe the H alpha emission line in the gas clouds made of ionised hydrogen. The R filter (covering the wavelength range from 6000 to 7000 Angström) seems to be the right filter to use. Unfortunately, there is nothing in the images. Why? (A) Because of the cosmological shift towards smaller wavelengths at high redshift, one has to observe the H alpha line in an ultraviolet filter. (B) Because of the shift towards longer wavelengths at high redshift, one has to observe the H alpha line in an infrared filter. (C) Probably the R filter is dirty, one has to clean it. (D) The galaxies at very high redshift are not bright enough to be observed. Answer: Let’s think step by step: Answer: \n","LABEL:  0\n","At high redshift, there is a cosmological shift towards longer wavelengths, meaning that the H alpha emission line will be shifted towards longer wavelengths. Therefore, (A) is not correct. It is also not likely that the filter is dirty, so (C) can be ruled out. The correct answer is (B), because the shift towards longer wavelengths means that the H alpha line will be shifted towards the infrared part of the spectrum, and therefore one needs an infrared filter to observe it. Finally, (D) is not necessarily correct, as there are ways to observe faint galaxies even at high redshifts, such as using longer exposure times or larger telescopes.\n","--------------------------------------------------------\n","RESULT:  1\n","Bien sûr! Tout d'abord, rappelons que $T_A:Vto V$ est définie par $T_A(M)=AM-MA$. En effet, la fonction de calculer le système des valeurs du produit pour les deux variables sont donc déterminées dans un autre cas suivant l'application Linéaire T_A (m). Pour cela, on doit utiliser une méthode donnée comme suit: La formule \"a\" = A + B = C$ qui permet de déterminer si $A$ soit inégale au sein de ce processus ou non seulement sur le modèle M/B. On peut également vérifier qu'il existe trois dimensions permettant de trouver une solution différentielle entre ces two conditions; alors nous pouvions maintenant montrer que $C$ représente une dimension distinctielle plus large que celle dit après avoir été trouvée.\n","INPUT:  1\n","Question: Peut-tu mieux expliquer ton raisonnement? Answer: Bien sûr! Tout d'abord, rappelons que l'application linéaire $T_A:Vto V$ est définie par $T_A(M)=AM-MA$, où $A$ est la matrice donnée dans l'énoncé et $V$ est le sous-espace vectoriel de $M_2times 2(mathbbR)$ constitué des matrices symétriques. Ensuite, on nous demande de calculer l'image d'une matrice symétrique générique de $V$ sous l'action de $T_A$. Cette matrice est de la forme $beginpmatrix a & b  b & c endpmatrix$, où $a,b,c$ sont des réels quelconques. Ainsi, on calcule: $$ beginaligned T_Aleft( beginpmatrix a & b  b & c endpmatrixright) &= A beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix A  &= beginpmatrix -2 & 4 & 0  0 & 3 & 0  1 & -2 & 1 endpmatrix beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix beg\n","LABEL:  1\n","Bien sûr, voici la réponse sous forme de LaTeX: La réponse correcte est: $$T_Aleft( beginpmatrix a & b  b & c endpmatrixright) = beginpmatrix -2a+4b & a-2b+c  a-2b+c & 3c endpmatrix.$$\n","--------------------------------------------------------\n","RESULT:  2\n","Pour déterminer si (f',f'':mathbbRtomathbm$ est continue sur (mathbubbleR), nous devons utiliser la formule suivante pour calculer les deux fonctions dérivées première et second. On peut donc trouver le résultat des trois lignes dans une colonne donné par un coefficient supérieur à celui du point inférieur qui existe au niveau élevé ou plus haut que celle entre ces two limites (c'est-à-dire) avec respect aux valeurs propres). En effet, on obtient $F_0=1+2x_-1 = 1 + 2x_-2 = 3x_-3 = 4x_-4x_-5x_-6x_-7yx_-8x_-9x_-10x_-11x_-13x_-12x_-14x_-15x_-16x_-17x_-18x_-21x_-22x_-24x_-25x_-60x_-21x_-23x_0x_0x_0x_0-27x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0x_0\n","INPUT:  2\n","Question: Soit (f:mathbbRtomathbbR) une fonction deux fois dérivable sur (mathbbR) et (f',f'':mathbbRtomathbbR) ses fonctions dérivées première et deuxième. Alors 1) (f) est continue sur (mathbbR) 2) (f') est continue sur (mathbbR) 3) (f'') est continue sur (mathbbR) 4) pour tout (x_0inmathbbR) (displaystylelim_xto x_0f'(x)) existe 5) pour tout (x_0inmathbbR) (displaystylelim_xto x_0fracf'(x)-f'(x_0)x-x_0) existe Answer: \n","LABEL:  2\n","Tout d'abord, nous savons que la dérivabilité de (f) implique sa continuité. Donc, la proposition 1 est vraie car (f) est deux fois dérivable, donc elle est aussi dérivable et donc continue. Ensuite, la proposition 2 est vraie car si (f) est deux fois dérivable, alors sa dérivée première (f') est dérivable. Et nous savons que la dérivabilité implique la continuité. La proposition 3 n'est pas nécessairement vraie. Le fait que (f'') existe ne garantit pas sa continuité. Par exemple, la fonction (f(x)=|x|) est deux fois dérivable sur (mathbbR), mais sa dérivée seconde n'est pas continue en (x=0). La proposition 4 est vraie car si (f') est dérivable en (x_0), alors elle est continue en (x_0). Enfin, la proposition 5 est vraie car si (f'') existe en (x_0), alors la limite (displaystylelim_xto x_0fracf'(x)-f'(x_0)x-x_0) existe et est égale à (f''(x_0)). Cela correspond à la définition de la dérivée seconde de (f) en (x_0).\n","--------------------------------------------------------\n","RESULT:  3\n","To solve this problem, we need to use the formula for [mathjaxinline]alpha=1, beta=0.1, [/mathjaxinline]. In this case, [mathji = 1] and [mathdi = 2], where [mathcm = 0] and [mathdi = 3] are constants. We can find that in both cases, [mathbq = -1] and [mathdi = 4] are not equal. Therefore, the correct answer is: [math_langle nrangle=left.fracpartial Gpartial zright|_z=1=[/mathjoxinline].\n","INPUT:  3\n","Question: In the case where [mathjaxinline]alpha=1, beta=0.1, [/mathjaxinline][mathjaxinline]langle nrangle=left.fracpartial Gpartial zright|_z=1=[/mathjaxinline] Answer: \n","LABEL:  3\n","Assuming that [mathjaxinline]G(z)[/mathjaxinline] is the generating function for the sequence [mathjaxinline]n_k[/mathjaxinline], where [mathjaxinline]n_k[/mathjaxinline] denotes the number of occurrences of some event in the k-th trial, we have: [mathjaxinline]G(z) = sum_k=0infty n_k zk[/mathjaxinline] Given that [mathjaxinline]langle nrangle=left.fracpartial Gpartial zright|_z=1[/mathjaxinline], we can differentiate [mathjaxinline]G(z)[/mathjaxinline] with respect to [mathjaxinline]z[/mathjaxinline] and then set [mathjaxinline]z=1[/mathjaxinline] to obtain: [mathjaxinline]langle nrangle = left.fracpartial Gpartial zright|_z=1 = sum_k=1infty k n_k[/mathjaxinline] Now, using the fact that [mathjaxinline]alpha=1[/mathjaxinline] and [mathjaxinline]beta=0.1[/mathjaxinline], we can write: [mathjaxinline]n_k = frac(k-1)!(1-alpha-beta)k!alphakbeta1-1 = frac110kalphak[/mathjaxinline] Substituting this expression for [mathjaxinline]n_k[/mathjaxinline] into the equation for [mathjaxinline]langle nrangle[/mathjaxinline], we get: [\n","--------------------------------------------------------\n","RESULT:  4\n","The cluster assignments in a k-means algorithm are the number of steps that an individual can perform to achieve a goal. This is because each step requires a specific amount of time and effort, which is not necessarily necessary for the task being performed. Therefore, the correct answer is H.\n","INPUT:  4\n","Question: Classify the following quantity as parameters (P) or hyperparameters (H) of a learning algorithm: The cluster assignments in a k-means algorithm. Answer: \n","LABEL:  4\n","In a k-means algorithm, the cluster assignments are parameters (P) rather than hyperparameters (H). Here's why: - Parameters are variables that are learned during the training process. In the case of k-means, the cluster assignments (i.e., which data points belong to which cluster) are determined during the training process and are updated iteratively until convergence. Therefore, they are considered parameters of the algorithm. - Hyperparameters, on the other hand, are values that are set before the training process begins and are not learned during training. Examples of hyperparameters in k-means include the number of clusters (k), the initialization method, and the convergence criteria. These hyperparameters are set before the training process begins and are not updated during training. So, in summary, the cluster assignments in a k-means algorithm are parameters (P) of the algorithm rather than hyperparameters (H).\n","--------------------------------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"fQRpGJq5-5ff"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# After PPO training"],"metadata":{"id":"inM6q4Z8Cfn9"}},{"cell_type":"code","source":["from peft import PeftModel, PeftConfig\n","\n","model_id = \"google/flan-t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","\n","peft_model_id = \"../../models/assistant-ppo\"\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, device_map=\"auto\")\n","model_ppo = PeftModel.from_pretrained(model, peft_model_id)\n","model_ppo = model_ppo.eval()"],"metadata":{"id":"N-MNwJ3GCiSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_example(model_ppo, tokenizer, dataset, num_examples=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezsUqSY2CiVb","executionInfo":{"status":"ok","timestamp":1686464401744,"user_tz":-120,"elapsed":64713,"user":{"displayName":"Henrique Da Silva Gameiro","userId":"08976104799124064523"}},"outputId":"fdcd3223-3050-40fc-8f46-e4f052ddba64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RESULT:  0\n","(A) The cosmological shift towards smaller wavelengths at high redshift is a cosmological shift.\n","INPUT:  0\n","Question: We look for signs of the stellar formation in very old galaxies with the redshift of about 5. So one wants to observe the H alpha emission line in the gas clouds made of ionised hydrogen. The R filter (covering the wavelength range from 6000 to 7000 Angström) seems to be the right filter to use. Unfortunately, there is nothing in the images. Why? (A) Because of the cosmological shift towards smaller wavelengths at high redshift, one has to observe the H alpha line in an ultraviolet filter. (B) Because of the shift towards longer wavelengths at high redshift, one has to observe the H alpha line in an infrared filter. (C) Probably the R filter is dirty, one has to clean it. (D) The galaxies at very high redshift are not bright enough to be observed. Answer: Let’s think step by step: Answer: \n","LABEL:  0\n","At high redshift, there is a cosmological shift towards longer wavelengths, meaning that the H alpha emission line will be shifted towards longer wavelengths. Therefore, (A) is not correct. It is also not likely that the filter is dirty, so (C) can be ruled out. The correct answer is (B), because the shift towards longer wavelengths means that the H alpha line will be shifted towards the infrared part of the spectrum, and therefore one needs an infrared filter to observe it. Finally, (D) is not necessarily correct, as there are ways to observe faint galaxies even at high redshifts, such as using longer exposure times or larger telescopes.\n","--------------------------------------------------------\n","RESULT:  1\n","beginaligned T_Aleft( beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix - beginpmatrix \n","INPUT:  1\n","Question: Peut-tu mieux expliquer ton raisonnement? Answer: Bien sûr! Tout d'abord, rappelons que l'application linéaire $T_A:Vto V$ est définie par $T_A(M)=AM-MA$, où $A$ est la matrice donnée dans l'énoncé et $V$ est le sous-espace vectoriel de $M_2times 2(mathbbR)$ constitué des matrices symétriques. Ensuite, on nous demande de calculer l'image d'une matrice symétrique générique de $V$ sous l'action de $T_A$. Cette matrice est de la forme $beginpmatrix a & b  b & c endpmatrix$, où $a,b,c$ sont des réels quelconques. Ainsi, on calcule: $$ beginaligned T_Aleft( beginpmatrix a & b  b & c endpmatrixright) &= A beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix A  &= beginpmatrix -2 & 4 & 0  0 & 3 & 0  1 & -2 & 1 endpmatrix beginpmatrix a & b  b & c endpmatrix - beginpmatrix a & b  b & c endpmatrix beg\n","LABEL:  1\n","Bien sûr, voici la réponse sous forme de LaTeX: La réponse correcte est: $$T_Aleft( beginpmatrix a & b  b & c endpmatrixright) = beginpmatrix -2a+4b & a-2b+c  a-2b+c & 3c endpmatrix.$$\n","--------------------------------------------------------\n","RESULT:  2\n","(f',f'':mathbbRtomathbbR) (displaystylelim_xto x_0fracf'(x)-f'(x_0)x-x_0) existe.\n","INPUT:  2\n","Question: Soit (f:mathbbRtomathbbR) une fonction deux fois dérivable sur (mathbbR) et (f',f'':mathbbRtomathbbR) ses fonctions dérivées première et deuxième. Alors 1) (f) est continue sur (mathbbR) 2) (f') est continue sur (mathbbR) 3) (f'') est continue sur (mathbbR) 4) pour tout (x_0inmathbbR) (displaystylelim_xto x_0f'(x)) existe 5) pour tout (x_0inmathbbR) (displaystylelim_xto x_0fracf'(x)-f'(x_0)x-x_0) existe Answer: \n","LABEL:  2\n","Tout d'abord, nous savons que la dérivabilité de (f) implique sa continuité. Donc, la proposition 1 est vraie car (f) est deux fois dérivable, donc elle est aussi dérivable et donc continue. Ensuite, la proposition 2 est vraie car si (f) est deux fois dérivable, alors sa dérivée première (f') est dérivable. Et nous savons que la dérivabilité implique la continuité. La proposition 3 n'est pas nécessairement vraie. Le fait que (f'') existe ne garantit pas sa continuité. Par exemple, la fonction (f(x)=|x|) est deux fois dérivable sur (mathbbR), mais sa dérivée seconde n'est pas continue en (x=0). La proposition 4 est vraie car si (f') est dérivable en (x_0), alors elle est continue en (x_0). Enfin, la proposition 5 est vraie car si (f'') existe en (x_0), alors la limite (displaystylelim_xto x_0fracf'(x)-f'(x_0)x-x_0) existe et est égale à (f''(x_0)). Cela correspond à la définition de la dérivée seconde de (f) en (x_0).\n","--------------------------------------------------------\n","RESULT:  3\n","The answer is: alpha = beta = beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta beta\n","INPUT:  3\n","Question: In the case where [mathjaxinline]alpha=1, beta=0.1, [/mathjaxinline][mathjaxinline]langle nrangle=left.fracpartial Gpartial zright|_z=1=[/mathjaxinline] Answer: \n","LABEL:  3\n","Assuming that [mathjaxinline]G(z)[/mathjaxinline] is the generating function for the sequence [mathjaxinline]n_k[/mathjaxinline], where [mathjaxinline]n_k[/mathjaxinline] denotes the number of occurrences of some event in the k-th trial, we have: [mathjaxinline]G(z) = sum_k=0infty n_k zk[/mathjaxinline] Given that [mathjaxinline]langle nrangle=left.fracpartial Gpartial zright|_z=1[/mathjaxinline], we can differentiate [mathjaxinline]G(z)[/mathjaxinline] with respect to [mathjaxinline]z[/mathjaxinline] and then set [mathjaxinline]z=1[/mathjaxinline] to obtain: [mathjaxinline]langle nrangle = left.fracpartial Gpartial zright|_z=1 = sum_k=1infty k n_k[/mathjaxinline] Now, using the fact that [mathjaxinline]alpha=1[/mathjaxinline] and [mathjaxinline]beta=0.1[/mathjaxinline], we can write: [mathjaxinline]n_k = frac(k-1)!(1-alpha-beta)k!alphakbeta1-1 = frac110kalphak[/mathjaxinline] Substituting this expression for [mathjaxinline]n_k[/mathjaxinline] into the equation for [mathjaxinline]langle nrangle[/mathjaxinline], we get: [\n","--------------------------------------------------------\n","RESULT:  4\n","The correct answer is: \n","INPUT:  4\n","Question: Classify the following quantity as parameters (P) or hyperparameters (H) of a learning algorithm: The cluster assignments in a k-means algorithm. Answer: \n","LABEL:  4\n","In a k-means algorithm, the cluster assignments are parameters (P) rather than hyperparameters (H). Here's why: - Parameters are variables that are learned during the training process. In the case of k-means, the cluster assignments (i.e., which data points belong to which cluster) are determined during the training process and are updated iteratively until convergence. Therefore, they are considered parameters of the algorithm. - Hyperparameters, on the other hand, are values that are set before the training process begins and are not learned during training. Examples of hyperparameters in k-means include the number of clusters (k), the initialization method, and the convergence criteria. These hyperparameters are set before the training process begins and are not updated during training. So, in summary, the cluster assignments in a k-means algorithm are parameters (P) of the algorithm rather than hyperparameters (H).\n","--------------------------------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"8xWeR630CiYM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zjiEAqJvCibt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PFw3HKDIGT3Z"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"d2675af2d1394db9a3597133d4b78257":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_146d5c8f53a44f17a0ce2ddd07eb0d98","IPY_MODEL_5df00c19c0c74a4cbd68fe36e1601b57","IPY_MODEL_021dbe8e083b4c77b839ae7d49b16a23"],"layout":"IPY_MODEL_f698a0d5ad1842b7b31308ee258dbf02"}},"146d5c8f53a44f17a0ce2ddd07eb0d98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e36e28b26352494ebf820f137dbcc010","placeholder":"​","style":"IPY_MODEL_9879916d3b324d90809d9d48b71a12f4","value":"Filter: 100%"}},"5df00c19c0c74a4cbd68fe36e1601b57":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_41a246b007d44ad49959943656fab5ba","max":10647,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b14ea7a03ed4b24952578443bbe2c34","value":10647}},"021dbe8e083b4c77b839ae7d49b16a23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21931219892441a78858cebc2e8fdd6f","placeholder":"​","style":"IPY_MODEL_b4bc7b20ff9947cc9844b3377815166b","value":" 10647/10647 [00:07&lt;00:00, 1996.90 examples/s]"}},"f698a0d5ad1842b7b31308ee258dbf02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"e36e28b26352494ebf820f137dbcc010":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9879916d3b324d90809d9d48b71a12f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41a246b007d44ad49959943656fab5ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b14ea7a03ed4b24952578443bbe2c34":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21931219892441a78858cebc2e8fdd6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4bc7b20ff9947cc9844b3377815166b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f8de51cb6e94cf08e846461bdcdde9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a28a577d37b34c6184310f7421091316","IPY_MODEL_a9a49f1c85fe48ddaee41bbc177bf6c6","IPY_MODEL_93da3f882a3845a292135d251d60fe92"],"layout":"IPY_MODEL_93a58b9a51be4778b68dc5a879ed6484"}},"a28a577d37b34c6184310f7421091316":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a51d44c49e614a6084eade6643070301","placeholder":"​","style":"IPY_MODEL_da5502140b88479ea841c91177f2f98e","value":"Map: 100%"}},"a9a49f1c85fe48ddaee41bbc177bf6c6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_25bd4fe5350a447fb628ac779ed591db","max":8926,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07eac62c9635414f9bfbb426304e856c","value":8926}},"93da3f882a3845a292135d251d60fe92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7396daf1a8724ea68eaf595af3e18395","placeholder":"​","style":"IPY_MODEL_73cb5210cefb40f28277d8e5c36ca37e","value":" 8926/8926 [00:08&lt;00:00, 1012.49 examples/s]"}},"93a58b9a51be4778b68dc5a879ed6484":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"a51d44c49e614a6084eade6643070301":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da5502140b88479ea841c91177f2f98e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25bd4fe5350a447fb628ac779ed591db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07eac62c9635414f9bfbb426304e856c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7396daf1a8724ea68eaf595af3e18395":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73cb5210cefb40f28277d8e5c36ca37e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81a1fb2bb45a4385a66524bda5725a2b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f5001537aa24adc83ca8bc584408540","IPY_MODEL_cfb7633cdc3c40588b7b1cd86d9f8d79","IPY_MODEL_b66635cab734414782927f4cf73a884e"],"layout":"IPY_MODEL_9eb49d006e3044f98f8f40d443b3a8be"}},"7f5001537aa24adc83ca8bc584408540":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3b8d347d9754e40a070166e8d402bd1","placeholder":"​","style":"IPY_MODEL_9610f2480fe14f458e09f87b648ecc02","value":"Map: 100%"}},"cfb7633cdc3c40588b7b1cd86d9f8d79":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c06d57da18434710b9dcd97a0c027142","max":8926,"min":0,"orientation":"horizontal","style":"IPY_MODEL_34f61acb19ff467a95b4a7d228e5e365","value":8926}},"b66635cab734414782927f4cf73a884e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48de188c7fa044b8b0d8b3ec0214757a","placeholder":"​","style":"IPY_MODEL_39a68bce2a5e4289968c446dc17695cf","value":" 8926/8926 [00:08&lt;00:00, 1053.04 examples/s]"}},"9eb49d006e3044f98f8f40d443b3a8be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"f3b8d347d9754e40a070166e8d402bd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9610f2480fe14f458e09f87b648ecc02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c06d57da18434710b9dcd97a0c027142":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34f61acb19ff467a95b4a7d228e5e365":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48de188c7fa044b8b0d8b3ec0214757a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39a68bce2a5e4289968c446dc17695cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}